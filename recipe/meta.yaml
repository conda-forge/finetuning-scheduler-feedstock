{% set name = "finetuning-scheduler" %}
{% set version = "2.0.9" %}

package:
  name: {{ name|lower }}
  version: {{ version }}

source:
  url: https://github.com/speediedan/finetuning-scheduler/releases/download/v{{ version }}/finetuning-scheduler-{{ version }}.tar.gz
  sha256: a14c1e60e21b32e0b2cc375e97a18c2923340f45e9663e99fabec0104d821148

build:
  script_env:
    - PACKAGE_NAME=pytorch
  number: 0
  noarch: python
  script: {{ PYTHON }} -m pip install . -vv

requirements:
  host:
    - pip
    - python >=3.8, <3.11 # TODO: this upper bound is because of PyTorch issue with dataclass
  run:
    - python >=3.8, <3.11 # TODO: this upper bound is because of PyTorch issue with dataclass
    - pytorch-lightning >=2.0.0, <=2.0.9
    - pytorch >=1.11

test:
  imports:
    - finetuning_scheduler
  requires:
    - pip

about:
  home: https://github.com/speediedan/finetuning-scheduler
  summary: A PyTorch Lightning extension that enhances model experimentation with flexible fine-tuning schedules.
  license: Apache-2.0
  license_file: LICENSE
  doc_url: https://finetuning-scheduler.readthedocs.io/en/stable/
  dev_url: https://github.com/speediedan/finetuning-scheduler
  description: |
    The FinetuningScheduler callback accelerates and enhances foundational model experimentation with flexible fine-tuning
    schedules. Training with the FinetuningScheduler callback is simple and confers a host of benefits:

    - it dramatically increases fine-tuning flexibility
    - expedites and facilitates exploration of model tuning dynamics
    - enables marginal performance improvements of finetuned models

    Fundamentally, the FinetuningScheduler callback enables multi-phase, scheduled fine-tuning of foundational models.
    Gradual unfreezing (i.e. thawing) can help maximize foundational model knowledge retention while allowing (typically
    upper layers of) the model to optimally adapt to new tasks during transfer learning.

    FinetuningScheduler orchestrates the gradual unfreezing of models via a fine-tuning schedule that is either implicitly
    generated (the default) or explicitly provided by the user (more computationally efficient). Fine-tuning phase
    transitions are driven by FTSEarlyStopping criteria (a multi-phase extension of EarlyStopping), user-specified epoch
    transitions or a composition of the two (the default mode). A FinetuningScheduler training session completes when the
    final phase of the schedule has its stopping criteria met.

    Documentation
    -------------
    - https://finetuning-scheduler.readthedocs.io/en/stable/
    - https://finetuning-scheduler.readthedocs.io/en/latest/

extra:
  recipe-maintainers:
    - speediedan
